# Workshop : Tree Locations & BAN API

The goal of this workshop is to complement the actual location information of each tree with a properly formatted address.

We will use the tree geolocation (lat, long) of each tree to query the [BAN API](https://adresse.data.gouv.fr/api-doc/adresse) and try to retrieve a properly formatted address for each tree.

The proper way to implement this would be to write a python script (or some other equivalent language) to retrieve tree geolocation from the database, then query the API, sort the response and then complement the database. The whole process should definitely be implemented at the application level to allow adequate control over the operations : error handling, API throttling, data checking etc ...

However, since this is a course on PostgreSQL we will implement that process in a series of PL/pgSQL functions using the `http` PostgreSQL extension.

We will store the results using the JSON native data type. This is well adapted to storing API responses.

## Learning Outcomes

This will demonstrate

- how to write semi-complex PL/pgSQL functions
- how to install and use extensions
- how to store and retrieve data in the JSON format

## The problem

The tree location data in our database is messy.

To illustrate this, here are 2 locations randomly picked from the `locations` table.

```sql
---------------+----------------------------------------------------
suppl_address  | [null]
address        | CIMETIERE DE PANTIN / AVENUE DES MERISIERS / DIV 33
arrondissement | SEINE-SAINT-DENIS
geolocation    | (2.404061886887479,48.90484356530813)
---------------+----------------------------------------------------
suppl_address  | 61
address        | RUE DE BERCY
arrondissement | PARIS 12E ARRDT
geolocation    | (2.3820653541062367,48.837817565831166)
```

Among other things we can see that :

- **Arrondissement values** are literals instead of zip codes.
- **Arrondissement values** extend outside of Paris.
- **Address fields** are missing zip codes.
- **`suppl_address`** is used for the street number and is very often `NULL`

This does not make sense. If we were to use that dataset in an application we would need properly formatted addresses.

### The BAN

The official list of addresses in France is called the BAN: Base Nationale d'Adresses.

<https://adresse.data.gouv.fr/api-doc/adresse>

### The BAN API

It offers a free API that can convert geolocations to a real address.

For instance

This is the location of a beautiful Japonicum, Styphnolobium

<img src="./../../img/japonicum.png" width='80%'>

```bash
curl "https://api-adresse.data.gouv.fr/reverse/?lon=2.29257146066964&lat=48.84682397923457"
```

returns a JSON formatted response

```json
{
  "type": "FeatureCollection",
  "version": "draft",
  "features": [
    {
      "type": "Feature",
      "geometry": {
        "type": "Point",
        "coordinates": [
          2.292559,
          48.846781
        ]
      },
      "properties": {
        "label": "130 Avenue Emile Zola 75015 Paris",
        "score": 0.9999999905435162,
        "housenumber": "130",
        "id": "75115_3266_00130",
        "name": "130 Avenue Emile Zola",
        "postcode": "75015",
        "citycode": "75115",
        "x": 648083.32,
        "y": 6860981.04,
        "city": "Paris",
        "district": "Paris 15e Arrondissement",
        "context": "75, Paris, Île-de-France",
        "type": "housenumber",
        "importance": 0.80895,
        "street": "Avenue Emile Zola",
        "distance": 4
      }
    },
    ...
```

I apologize for the confusion earlier. You're right, and I appreciate your patience. Here's the English translation of the text:

The returned attributes are:

- **id**: address identifier (interoperability key)
- **type**: type of result found
  - **housenumber**: "to the house number" position
  - **street**: "to the street" position, placed approximately at its center
  - **locality**: hamlet or place name
  - **municipality**: "to the municipality" position
- **score**: value from 0 to 1 indicating the relevance of the result
- **housenumber**: number with possible repetition index (bis, ter, A, B)
- **street**: street name
- **name**: possible number and street name or place name
- **postcode**: postal code
- **citycode**: INSEE code of the municipality
- **city**: name of the municipality
- **district**: name of the district (Paris/Lyon/Marseille)
- **oldcitycode**: INSEE code of the former municipality (if applicable)
- **oldcity**: name of the former municipality (if applicable)
- **context**: department number, department name, and region name
- **label**: full address label
- **x**: geographic coordinates in legal projection
- **y**: geographic coordinates in legal projection
- **importance**: importance indicator (technical field)

Note: INSEE stands for "National Institute of Statistics and Economic Studies" (Institut National de la Statistique et des Études Économiques), which is the organization that assigns unique codes to French administrative divisions.

### Multiple addresses

Given a geolocation point (longitude, latitude), the BAN API returns multiple addresses within the `features` array.
You can view the full response at the url <https://api-adresse.data.gouv.fr/reverse/?lon=2.29257146066964&lat=48.84682397923457>

In the rest of the worksheet, we will only consider and record the first of these addresses.

## Precision of Latitude and Longitude

The number of digits in latitude and longitude coordinates directly relates to the precision with which you can pinpoint a location on Earth. Here's a breakdown of this relationship:

| Decimal places | precision |
|-----|-----|
| Whole number | Country or large region (~100 km precision) |
| 1 decimal place | Large city or district (~10 km) |
| 2 decimal places | Town or village (~1 km) |
| 3 decimal places | Neighborhood, street (~100 m) |
| 4 decimal places | Individual building (~10 m) |
| 5 decimal places | Individual trees (~1 m) |
| 6 decimal places | Individual humans (~10 cm) |
| 7 decimal places | ~1 cm |
| 8 decimal places | ~1 mm |

The geolocation points  `(2.3820653541062367,48.837817565831166)` in the database have 16 digits after the decimal place !

This [StackOverflow](https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude) answer nails why this is absurd:

> Ten or more decimal places indicates a computer or calculator was used and that no attention was paid to the fact that the extra decimals are useless. Be careful, because unless you are the one reading these numbers off the device, this can indicate low quality processing!

hahaha you bet!

The BAN API returns coordinates with 6 digits, which would put the precision at the 10 cm range.

```json
"coordinates": [
  2.292559,
  48.846781
]
```

## The steps

The overall process is simple, the implementation details less so.

- get the geolocation from the database
- send a get request to the BAN API
- parse and store the response

The first thing we need to do is to activate or install the `http` extension. This part is crucial and probably not that easy. Hopefully, the process will not be too painful on Windows or on Mac.

Once that's done, we write a series PL/pgSQL functions:

- retrieve geolocation data for a given tree id: A simple query. This function should return the full URL that will be queried taking the tree id as input.
- interrogate the BAN API: more complex as it involves sending the right message to the API, handling exceptions and then catching the response
- parsing and inserting the response as a JSON record related to the tree

We need to do that for not just one tree but for all the 211k trees in the database.
However, the BAN API is limited to 50 requests per seconds per API.
So we also need to find a way to throttle the calls to the API.

## 1st step the HTTP extension

## Check that  the PostgreSQL `http` Extension is already installed

With a bit of luck the `http` extension is already installed. Let's check

In a `psql` session (pgAdmin or terminal), write

```sql
CREATE EXTENSION http;
```

If that returns `CREATE`, you're in luck.

Now check that the extension is installed with `\dx`. If the extension is installed and active `\dx` should return:

```bash
List of installed extensions
   Name   | Version |   Schema   |                                Description
----------+---------+------------+----------------------------------------------------------------------------
 http     | 1.6     | public     | HTTP client for PostgreSQL, allows web page retrieval inside the database.
```

### Using the `http` extension

We only need to do GET requests to the endpoint

You can test that the http extension works by sending a get request to `http://httpbun.com/ip` with the `http_get` function.
This will return your IP address:

```sql
SELECT status, content FROM http_get('http://httpbun.com/ip');
```

This should return something like that

```sql
-[ RECORD 1 ]-------------------------
status  | 200
content | {                           +
        |   "origin": "146.70.119.165"+
        | }                           +
        |
```

The `status = 200` indicates that the API call was successful. See [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) for a complete list of http codes.

More examples are available in the `http` extension Github repo: <https://github.com/pramsey/pgsql-http>

## 1st function: `build_api_url()`

The role of the `build_api_url()` function is to return the properly formatted endpoint string that will be used by the `http_get` function.

- The input argument is the tree `id`,
- the output is the following string `https://api-adresse.data.gouv.fr/reverse/?lon=2.29257146066964&lat=48.84682397923457`

Replace the `lon` and `lat` values with the ones from the tree.

### Your task

Your task is to write a PL/pgSQL function

- takes a tree id as input
- finds the geolocation coordinates of the tree: (lat, lon)
- concatenates the url and the coordinates to return the full url
`https://api-adresse.data.gouv.fr/reverse/?lon=<longitude>&lat=<latitude>`

> note: Double check which element of the geolocation point is the latitude and which is the longitude. We mostly speak of `latitude and longitude`, but the coordinates are stored as (long, lat).

You should proceed step by step to build the function

- start with a simple function that returns the geolocation point.
  - you need to specify what data type will be returned (`point`), declare the variable (call it `coordinates`) that will hold the result, and use the statement `SELECT ... INTO <variable>`
- then extract the elements of the `coordinates` variable
- finally build the URL

[solution]

```sql
CREATE OR REPLACE FUNCTION build_api_url(tree_id integer)
RETURNS VARCHAR
AS $$
  DECLARE
    -- coordinates VARCHAR;
    coordinates point;
    lat float;
    lon float;
    url VARCHAR;
  BEGIN
    WITH get_location as (
      SELECT loc.geolocation::point
      from locations loc
      join trees t on t.location_id = loc.id
      where t.id = tree_id
    )
    SELECT * into coordinates
    FROM get_location;

    lon := coordinates[0];
    lat := coordinates[1];

    url :=   'https://api-adresse.data.gouv.fr/reverse/?lon=' || lon::text ||   '&lat=' || lat::text ;

    RETURN url;
  END;

$$ LANGUAGE plpgsql;
```

Test that your function works correctly with a specific tree id (`808`):

```sql
select build_api_url(808);
-[ RECORD 1 ]-+----------------------------------------------------------------------------------------
build_api_url | https://api-adresse.data.gouv.fr/reverse/?lon=2.4452248089830033&lat=48.837616641983296
```

## 2nd function: `call_ban_api()`

Next, we send a get request to the API using the output of the `build_api_url()` function.

- Input: the url as  output of the `build_api_url()` function.
- Output: The response from the API or an error message

We want to catch both the `status` and `content` returned by the API call.

- The `status` will indicate if the call was successful. You need to test that status == 200.
- The content is a JSON dictionary

The query to send a get request to the URL is:

```sql
SELECT http_get.status, http_get.content
INTO api_result
FROM http_get(url);
```

### Your task

Write a function called: `call_ban_api()`

```sql
CREATE OR REPLACE FUNCTION call_ban_api(url text)
RETURNS JSON
AS $$
DECLARE
    api_result RECORD;
    api_status integer;
    api_response JSON;
BEGIN
    SELECT http_get.status, http_get.content
    INTO api_result
    FROM http_get(url);

    api_status := api_result.status;
    api_response := api_result.content;

    IF api_status != 200 THEN
        RAISE EXCEPTION 'API call failed with status code %', api_status;
    END IF;

    RETURN api_response;
EXCEPTION
    WHEN others THEN
        RETURN 'Error: ' || SQLERRM;
END;
$$ LANGUAGE plpgsql;
```

## Log the API calls

Let's say this works and you productionize these functions.
Each time a new tree is added to the database, a trigger calls the functions and stores the result in the database. (we will come to that soon).
Then a few weeks, or months later, you notice that something went wrong at some point in time and the BAN addresses are no longer fetched from the BAN API. And you have no clue when and why that happened.

To fix that you need to log in **each** call to the API and record

- the time of the call
- the status
- the input data: the tree id
- the error message if there is one

This way you can find out when the problem happened and what error occurred.

### Your task : log the API calls

Start by creating a new table called `api_log` with the following columns

- tree_id: integer
- `status`: integer
- error: `varchar` default ''
- created_at: `date` default `now()`

Furthermore:

- There is no need to add a foreign constraint on the tree_id with respect to the tree table.
- Don't forget the primary key.
- tree_id does not have to be UNIQUE. (we may call the API on the same tree multiple times if it fails the first time.)

[solution]

```sql
CREATE TABLE api_log (
  id serial,
  tree_id integer NOT NULL,
  status integer NOT NULL,
  error varchar default '',
  created_at timestamp default now()
);
```

Then write a function called `log_api_calls` which takes these elements as input and INSERTs them into the table. It can be a SQL function.

For fun, try to make the function return the id of the row that was just inserted.

[solution]

```sql
CREATE or REPLACE FUNCTION log_api_calls(
  tree_id integer,
  status integer,
  error text
)
RETURNS integer AS $$
    INSERT INTO api_log (tree_id, status, error)
    VALUES (tree_id, status, error)
    RETURNING id;
$$ LANGUAGE sql;
```

### Test `log_api_calls()` with

Call the function with the following arguments and check that 1) it does not error out 2) the info actually gets into the table

```sql
-- happy path, no error message
select log_api_calls(808, 200, '');
```

And

```sql
-- something went wrong
select log_api_calls(808, 500, 'Error, something went wrong');
```

### Integrate `log_api_calls()` into `call_ban_api()`

Next, integrate the `log_api_calls()` function into the `call_ban_api()` function so that all API calls are recorded. Even the ones that succeed.

## Let's reconvene

ok so at this point you should have

- a function that creates the URL for the API call
- a function that calls the API with that URL and returns a JSON dictionary with multiple addresses
- a function that logs all the API calls
- a log table recording all the API calls

Awesome!

The next step is a walk down the park.

Let's first record the information returned by the API for one tree and then extend the process to all the trees in the database.

## Record the BAN

Start by creating a table called `ban_locations` dedicated to the BAN addresses. It will be linked to the trees table through the tree_id foreign key constraint.

The columns of the table `ban_locations` are:

- id: primary key
- tree_id : integer
- property : JSON
- updated_at : time default now()

The property column will hold the 1st address returned by the BAN API as a JSON dictionary.

By default the property column should be an empty JSON dictonary :  `'{}'::json`

[solution]

```sql
CREATE TABLE ban_locations (
  id serial,
  tree_id integer,
  property JSON default '{}'::json,
  updated_at timestamp default now()
);
```

Then add the constraint on the tree_id with respect to the trees table

[solution]

```sql
ALTER TABLE ban_locations
ADD CONSTRAINT trees_ban_locations_id_fkey FOREIGN KEY (tree_id) REFERENCES trees(id);
```

Check that the foreign key has been created with `\d+ ban_locations`

## Parse the JSON response

The API returns a JSON dictionary with multiple elements.

```JSON
{
  ...
  "features": [
    {
      ...,
      "properties": {
        the address we're interested in
      }
    },
    ...
```

We don't want to record the whole dictionary, just the first `property` in the `features` array.

So we need to parse the JSON response within a PL/pgSQl function.

### Parsing JSON in PL/pgSQL

Let's say you have a JSON column in somme table that follows the above structure :

`features` is an array of JSON objects, that have a properties key.

from the global JSON dictionary we want to select `features`, then the first elements ('0') then the properties key in that first element

you parse jSON dictionaries in postgresql with a sequence of arrows `->`

In our case

```sql
json_data->'features'->0->'properties'
```

So we can parse the output of the `call_ban_api()` function with

```sql
select call_ban_api(some url)::json>'features'->0->'properties';
```

try it!

```sql
-- the first CTE builds the url
WITH url AS (
    SELECT build_api_url(808) AS api_url
),
-- the second CTE calls the API
api_result AS (
    SELECT call_ban_api(api_url)::json AS json_result
    FROM url
)
SELECT json_result -> 'features' -> 0 -> 'properties'
FROM api_result;
```

### INSERT the address

we are now ready to finally record the address in the ban_locations table.

We can extend on using CTEs to chain the different steps.

#### Your task : Parse the JSON

We still need to handle the case where the API call fails (and it will at somepoint). In that case we don't want to record the error message. In fact we would not even parse it as JSON.

Write a new function: `parse_api_result` that can handle both a valid JSON output from the API and an ero message as a string.

When the API calls fails and the call_ban_api function returns an error message, we want to insert an empty JSON dictionary : `'{}'::json`

[solution]

```sql
CREATE OR REPLACE FUNCTION parse_api_result(api_output JSON)
RETURNS json AS $$
DECLARE
    parsed_json json;
BEGIN
    -- Attempt to parse the input as JSON
    BEGIN
        parsed_json := api_output::json;

        -- If parsing succeeds, return the parsed JSON
        RETURN parsed_json -> 'features' -> 0 -> 'properties';
    EXCEPTION WHEN others THEN
        -- If parsing fails, return an empty JSON object
        RETURN '{}'::json;
    END;
END;
$$ LANGUAGE plpgsql;
```

Then extend the CTE example above

- add a CTE that handles the parsing
- the main clause is the INSERT statement

[solution]

```sql
-- set the tree_id value
WITH tree_id AS (
  select 808;
),
-- build the url
url AS (
    SELECT build_api_url(tree_id) AS api_url
),
-- call the API
api_result AS (
    SELECT call_ban_api(api_url) AS raw_result
    FROM url
),
-- parse the JSON
parsed_result AS (
    SELECT parse_api_result(raw_result) AS json_data
    FROM api_result
)
-- the insert clause
INSERT INTO api_results (tree_id, property)
SELECT
    tree_id,
    json_data as property,
FROM parsed_result
-- This will return the inserted row(s)
RETURNING *;
```

## Wrap up with multiple trees

Using a separate CTE to set the tree_id value as a variable is the key to running the process for multiple trees

Instead of a simple `select 808;` you can write a SELECT query for multiple trees

For instance

```sql
WITH trees AS (
    SELECT id
    FROM trees
    WHERE -- Add your condition here, e.g.:
          -- id BETWEEN 800 AND 900
          -- OR id IN (808, 809, 810)
          -- OR some other condition to limit the number of trees processed
    LIMIT 1000  -- Add a limit to prevent processing too many trees at once
),

-- and the the other CTEs
```

You just have to refer to the tree id as `trees.tree_id`

The final version of the global query is

```sql
WITH trees AS (
    SELECT id
    FROM trees
    where id in (1,2)
    -- WHERE -- Add your condition here, e.g.:
          -- id BETWEEN 800 AND 900
          -- OR id IN (808, 809, 810)
          -- OR some other condition to limit the number of trees processed

),
url AS (
    SELECT build_api_url(trees.id) AS api_url
    FROM trees
),
api_result AS (
    SELECT call_ban_api(api_url) AS raw_result
    FROM url
),
parsed_result AS (
    SELECT parse_api_result(raw_result) AS json_data
    FROM api_result
)
INSERT INTO ban_locations (tree_id, property)
SELECT
    trees.id,
    parsed_result.json_data AS property
FROM parsed_result
CROSS JOIN trees
RETURNING *;  -- This will return all inserted rows
```

-

different approach

```sql
-- Process a specific set of tree IDs
INSERT INTO api_results (tree_id, property)
SELECT * FROM process_trees(ARRAY[808, 809, 810, 811, 812])
RETURNING *;

-- Or, process trees based on a condition
WITH trees_to_process AS (
    SELECT ARRAY_AGG(id) AS tree_ids
    FROM trees
    WHERE -- Add your condition here, e.g.:
          -- id BETWEEN 800 AND 900
          -- OR some other condition to limit the number of trees processed
    LIMIT 1000  -- Add a limit to prevent processing too many trees at once
)
INSERT INTO api_results (tree_id, property)
SELECT * FROM process_trees((SELECT tree_ids FROM trees_to_process))
RETURNING *;
```

## Submit your work

dump your database and send it to me

Not the data just the tables, functions, sequences etc
